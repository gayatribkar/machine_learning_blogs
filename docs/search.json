[
  {
    "objectID": "posts/Anomaly detection/index.html",
    "href": "posts/Anomaly detection/index.html",
    "title": "Exploring Unsupervised Anomaly Detection in Thyroid Dataset (BLOG ON ANOMALY/OUTLIER DETECTION)",
    "section": "",
    "text": "Anomaly detection is a crucial aspect of data analysis, helping us identify patterns that deviate significantly from the norm. In this blog post, our primary objective is to employ sophisticated anomaly detection techniques to identify and visualize outliers within the dataset. We will explore the world of unsupervised anomaly detection in the context of thyroid dataset.\nUnderstanding the Dataset\nOur blog begins with loading and understanding the dataset. After removing unnecessary columns, we delve into the visualization to gain insights into the distribution of data points.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\ndata = pd.read_csv('annthyroid_unsupervised_anomaly_detection.csv', delimiter=';')\n\ndata.head()\n\n\n\n\n\n\n\n\nAge\nSex\non_thyroxine\nquery_on_thyroxine\non_antithyroid_medication\nsick\npregnant\nthyroid_surgery\nI131_treatment\nquery_hypothyroid\n...\nhypopituitary\npsych\nTSH\nT3_measured\nTT4_measured\nT4U_measured\nFTI_measured\nOutlier_label\nUnnamed: 22\nUnnamed: 23\n\n\n\n\n0\n0.45\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n61.0\n6.0\n23.0\n87.0\n26.0\no\nNaN\nNaN\n\n\n1\n0.61\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n29.0\n15.0\n61.0\n96.0\n64.0\no\nNaN\nNaN\n\n\n2\n0.16\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n29.0\n19.0\n58.0\n103.0\n56.0\no\nNaN\nNaN\n\n\n3\n0.85\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n114.0\n3.0\n24.0\n61.0\n39.0\no\nNaN\nNaN\n\n\n4\n0.75\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n49.0\n3.0\n5.0\n116.0\n4.0\no\nNaN\nNaN\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n# Checking the shape and data types of the dataset\nprint(data.shape)\n\n(6916, 24)\n\n\n\n# Descriptive statistics of the dataset\ndata.describe()\n\n\n\n\n\n\n\n\nAge\nSex\non_thyroxine\nquery_on_thyroxine\non_antithyroid_medication\nsick\npregnant\nthyroid_surgery\nI131_treatment\nquery_hypothyroid\n...\ntumor\nhypopituitary\npsych\nTSH\nT3_measured\nTT4_measured\nT4U_measured\nFTI_measured\nUnnamed: 22\nUnnamed: 23\n\n\n\n\ncount\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n...\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n6916.000000\n0.0\n0.0\n\n\nmean\n0.595098\n0.307548\n0.134615\n0.015616\n0.013158\n0.038317\n0.011278\n0.014315\n0.016773\n0.062753\n...\n0.025448\n0.000145\n0.049740\n1.603466\n11.772446\n94.253834\n88.270139\n95.257058\nNaN\nNaN\n\n\nstd\n6.189326\n0.461512\n0.341337\n0.123993\n0.113959\n0.191974\n0.105606\n0.118793\n0.128428\n0.242536\n...\n0.157494\n0.012025\n0.217422\n14.047218\n11.836250\n50.555642\n33.909845\n55.031137\nNaN\nNaN\n\n\nmin\n0.010000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000500\n0.002500\n0.050000\n0.002400\nNaN\nNaN\n\n\n25%\n0.370000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000680\n0.020100\n77.000000\n83.000000\n81.000000\nNaN\nNaN\n\n\n50%\n0.540000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.001700\n13.000000\n101.000000\n96.000000\n105.000000\nNaN\nNaN\n\n\n75%\n0.670000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.002700\n22.000000\n123.000000\n104.000000\n125.000000\nNaN\nNaN\n\n\nmax\n515.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n494.000000\n95.000000\n395.000000\n233.000000\n642.000000\nNaN\nNaN\n\n\n\n\n8 rows × 23 columns\n\n\n\n\n# Drop unnecessary columns\ndata = data.drop([\"Unnamed: 22\", \"Unnamed: 23\"], axis=1)\n\n# Select numerical columns for outlier detection\nnumerical_cols = ['TSH', 'T3_measured', 'TT4_measured', 'T4U_measured', 'FTI_measured']\n\n# Create a new DataFrame with only numerical columns\ndata_numerical = data[numerical_cols]\n\n# Visualize the correlation matrix\nsns.heatmap(data[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap of Numerical Features')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(15, 8))\nfor i, col in enumerate(numerical_cols):\n    plt.subplot(2, 3, i+1)\n    plt.hist(data[col], bins=20, color='skyblue', edgecolor='black')\n    plt.title(f'Histogram of {col}')\nplt.tight_layout()\nplt.show()\n\n\n\n\nTraditional Outlier Detection: Boxplots and Violin Plots\nNext, we employ traditional methods to identify outliers in the dataset. Boxplots provide a quick summary of the distribution of each feature, making it easy to spot potential outliers. Violin plots, on the other hand, combine the benefits of box plots and kernel density plots, offering a richer visualization of the data distribution.\n\nplt.figure(figsize=(9, 5))\ndata.boxplot(column=numerical_cols)\nplt.title('Boxplot for Numerical Columns')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(9, 5))\nsns.violinplot(data=data[numerical_cols], palette='husl')\nplt.title('Violin Plot of Numerical Features')\nplt.show()\n\n\n\n\nIsolation Forest: An Ensemble Method\nTo enhance our anomaly detection capabilities, we introduce the Isolation Forest algorithm. By fitting the model to numerical columns, we identify outliers and visualize them using a scatter plot. The outliers are then removed.\n\nfrom sklearn.ensemble import IsolationForest\n\n# Select numerical columns for outlier detection\nnumerical_cols = ['TSH', 'T3_measured', 'TT4_measured', 'T4U_measured', 'FTI_measured']\n\n# Create a new DataFrame with only numerical columns\ndata_numerical = data[numerical_cols]\n\n# Fit Isolation Forest model\nclf = IsolationForest(contamination=0.1, random_state=42)\ndata['IsolationForest_Outlier'] = clf.fit_predict(data_numerical)\n\n# Visualize outliers\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='TSH', y='T3_measured', hue='IsolationForest_Outlier', data=data, palette='viridis')\nplt.title('Outliers Visualization')\n\nplt.show()\n\n# Remove Isolation Forest outliers\ndata_no_iso_outliers = data[data['IsolationForest_Outlier'] == 1]\n\n\n\n\nOur RFM data now provides insights into how recently customers made a purchase, how frequently they buy, and how much money they spend.\nZ-Score Method: A Statistical Approach\nThe Z-score method is applied to identify and visualize outliers, followed by the removal of outliers. We calculate Z-scores for numerical columns and set a threshold for outlier identification. Visualizing outliers using a scatter plot and removing them from the dataset, we once again showcase the impact of this technique through a pair plo\n\nfrom scipy.stats import zscore\n\n# Calculate Z-scores for numerical columns\nz_scores = zscore(data[numerical_cols])\n\n# Define a threshold for Z-score (e.g., 3 standard deviations)\nthreshold = 3\n\n# Identify outliers based on Z-scores\noutliers_zscore = (abs(z_scores) &gt; threshold).any(axis=1)\n# outliers = (abs(z_scores) &gt; threshold).any(axis=1)\ndata['ZScore_Outlier'] = outliers_zscore.astype(int)\n# Visualize outliers\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='TSH', y='T3_measured', hue=\"ZScore_Outlier\", data=data, palette='viridis')\nplt.title('Outliers Visualization using Z-Score')\nplt.show()\n\n# Remove Z-Score outliers\ndata_no_zscore_outliers = data[data['ZScore_Outlier'] == 1]\n\n\n\n\nThe below plot is a 3D scatter plot side by side. Each point represents a data instance, and the color indicates whether it is detected as an outlier by Isolation Forest or Z-Score. This visualization allows us to observe outliers in a three-dimensional space, providing insights into their distribution across multiple features.\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a 3D scatter plot for Isolation Forest outliers\nfig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(121, projection='3d')\nax1.scatter(data['TSH'], data['T3_measured'], data['TT4_measured'], c=data['IsolationForest_Outlier'], cmap='viridis')\nax1.set_title('3D Scatter Plot with Isolation Forest Outliers')\nax1.set_xlabel('TSH')\nax1.set_ylabel('T3_measured')\nax1.set_zlabel('TT4_measured')\n\n# Create a 3D scatter plot for Z-Score outliers\nax2 = fig.add_subplot(122, projection='3d')\nax2.scatter(data['TSH'], data['T3_measured'], data['TT4_measured'], c=data['ZScore_Outlier'], cmap='viridis')\nax2.set_title('3D Scatter Plot with Z-Score Outliers')\nax2.set_xlabel('TSH')\nax2.set_ylabel('T3_measured')\nax2.set_zlabel('TT4_measured')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nConclusion\nThis exploration into unsupervised anomaly detection in thyroid datasets showcases a range of techniques, from traditional methods to advanced algorithms. Each method contributes to a deeper understanding of the dataset, aiding in the identification and handling of outliers.\nBy integrating code with visualization, this approach empowers data analysts and scientists to make informed decisions in preprocessing and analysis. Choose the method that aligns with your dataset characteristics and analytical goals, ensuring a robust and reliable foundation for subsequent analyses.\n\n  –&gt;\n\n\nConnect with me on GitHub\n\n\nFind more projects and articles on my GitHub."
  },
  {
    "objectID": "posts/3. Regression/index.html",
    "href": "posts/3. Regression/index.html",
    "title": "A Comprehensive Analysis of Linear and Non-Linear Regression in Advertising Data (BLOG ON LINEAR AND NON LINEAR REGRESSION)",
    "section": "",
    "text": "In the dynamic world of marketing and advertising, gaining insights into the influence of diverse channels on sales performance is crucial for strategic decision-making. The advertising dataset serves as a valuable repository, unveiling details about budget allocations across pivotal platforms like TV, radio, and newspapers, juxtaposed against their respective impacts on sales. This information empowers marketers to discern the most effective channels, optimize resource allocation, and refine strategies, ultimately enhancing the overall return on investment and fostering data-driven precision in the dynamic landscape of advertising and sales optimization.\nExploring the customer behavior dataset\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Load the dataset\ndf1 = pd.read_csv(\"Advertising.csv\")\ndf1.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n1\n230.1\n37.8\n69.2\n22.1\n\n\n1\n2\n44.5\n39.3\n45.1\n10.4\n\n\n2\n3\n17.2\n45.9\n69.3\n9.3\n\n\n3\n4\n151.5\n41.3\n58.5\n18.5\n\n\n4\n5\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\nThe dataset comprises information about the advertising budget for TV, radio, and newspaper, along with the corresponding sales figures. To start our analysis, we will first drop the unnecessary ‘Unnamed:0’ column.\n\ndf = df1.drop(df1.columns[0], axis=1)\ndf.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\n\n# Checking the shape and data types of the dataset\nprint(df.shape)\n\n(200, 4)\n\n\n\n# Descriptive statistics of the dataset\ndf.describe()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n147.042500\n23.264000\n30.554000\n14.022500\n\n\nstd\n85.854236\n14.846809\n21.778621\n5.217457\n\n\nmin\n0.700000\n0.000000\n0.300000\n1.600000\n\n\n25%\n74.375000\n9.975000\n12.750000\n10.375000\n\n\n50%\n149.750000\n22.900000\n25.750000\n12.900000\n\n\n75%\n218.825000\n36.525000\n45.100000\n17.400000\n\n\nmax\n296.400000\n49.600000\n114.000000\n27.000000\n\n\n\n\n\n\n\nVisualizing the Data Distribution\nUnderstanding the distribution of each feature in the dataset is crucial for gaining insights. Let’s create histograms to visualize the distribution of TV, radio, and newspaper budgets.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(9, 5))\nfor i, variable in enumerate(df1.iloc[:, 1:]):\n    plt.subplot(2, 2, i + 1)\n    sns.histplot(df1[variable], kde=True, bins=20)\n    plt.title(f'Distribution of {variable}')\n    plt.tight_layout()\n\nplt.show()\n\n\n\n\nThese histograms provide a quick overview of the distribution of advertising budgets across different channels. It’s interesting to note the varying ranges and patterns in the data.\n\n# Scatter plot for TV vs Sales\nplt.figure(figsize=(6, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(df['TV'], df['Sales'], color='blue', alpha=0.7)\nplt.title('TV Budget vs Sales')\nplt.xlabel('TV Budget')\nplt.ylabel('Sales')\n\n# Scatter plot for Radio vs Sales\nplt.subplot(1, 3, 2)\nplt.scatter(df['Radio'], df['Sales'], color='green', alpha=0.7)\nplt.title('Radio Budget vs Sales')\nplt.xlabel('Radio Budget')\nplt.ylabel('Sales')\n\n# Scatter plot for Newspaper vs Sales\nplt.subplot(1, 3, 3)\nplt.scatter(df['Newspaper'], df['Sales'], color='red', alpha=0.7)\nplt.title('Newspaper Budget vs Sales')\nplt.xlabel('Newspaper Budget')\nplt.ylabel('Sales')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nThis set of three scatter plots visually illustrates the relationship between advertising budgets allocated to TV, radio, and newspaper channels and their respective impacts on sales. Each subplot reveals the distribution of sales across different budget ranges, providing a clear snapshot of the potential associations between advertising investments in each channel and resulting sales figures.\nExploring Correlations\nTo understand the relationships between variables, a correlation matrix is constructed and visualized using a heatmap.\n\ncorrelation_matrix = df.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\nThis heatmap helps us identify correlations between different advertising channels and sales. It becomes apparent that TV has a higher correlation with sales compared to radio and newspaper.\nLinear Regression Analysis\nMoving on to predictive analytics, we split the data into training and testing sets and perform linear regression analysis.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Split the data\nX = df.drop(['Sales'], axis=1)\ny = df['Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}')\n\nMean Squared Error: 3.1740973539761033\n\n\nThe mean squared error gives us an indication of the model’s performance. Lower values signify a better fit.\n\nfig = plt.figure(figsize=(10, 9))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X_test['Newspaper'], X_test['TV'], y_test, c='black', marker='o', label='Actual')\nax.scatter(X_test['Newspaper'], X_test['TV'], y_pred, c='blue', marker='^', label='Predicted')\n\nax.set_xlabel('Newspaper Budget')\nax.set_ylabel('Television Budget')\nax.set_zlabel('Sales')\nax.legend()\nax.set_title('Multivariate Linear Regression Predictions vs Actual')\n\nplt.show()\n\n\n\n\nThe above 3D scatter plot contrasts actual sales with predictions from a Multivariate Linear Regression model, revealing the model’s efficacy in capturing the impact of Newspaper and Television advertising budgets on sales. The visual distinction between actual and predicted values offers a concise evaluation of the model’s accuracy in multivariate sales forecasting.\nPolynomial Regression Analysis\nPolynomial regression is a powerful analytical technique employed to model relationships between variables that may not be linear. Imagine data points forming curves or intricate patterns rather than following a straightforward path. Polynomial regression allows us to capture these non-linear relationships by introducing higher-degree terms in our model. To capture potential non-linear relationships in our data, we explore polynomial regression with different degrees.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndegrees = np.arange(1, 11)\ncosts = []\n\nfor degree in degrees:\n    poly = PolynomialFeatures(degree=degree)\n    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly = poly.transform(X_test)\n\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train)\n    y_pred = model.predict(X_test_poly)\n\n    mse_poly = mean_squared_error(y_test, y_pred)\n    costs.append(mse_poly)\n\n# Plot the cost for each degree\nplt.xlim([0, 10])\nplt.ylim([0, 10])\nplt.plot(degrees, costs, marker='o')\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Mean Squared Error (Cost)')\nplt.title('Polynomial Regression Analysis')\nplt.show()\n\n\n\n\nThe plot reveals that a polynomial degree of 3 yields the minimum mean squared error, indicating a good compromise between bias and variance.\nNeural Network Regression\nFor a more complex model, we delve into neural network regression using TensorFlow. Neural network regression, involves constructing a multilayer perceptron capable of learning complex patterns for predicting continuous outputs. Utilizing ReLU activation functions for non-linearity, the model excels in capturing intricate relationships. Inspired by the human brain, neural networks undergo training to optimize weights, making them adept at nonlinear regression tasks.\n\nimport tensorflow as tf\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(tf.keras.layers.Dense(10, activation='relu'))\nmodel.add(tf.keras.layers.Dense(10, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='linear'))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nhistory = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Neural Network Loss vs Epochs')\nplt.legend()\nplt.show()\n\nneural_pred = model.predict(X_test)\n\nmse_neural = mean_squared_error(neural_pred, y_test)\n\nprint(f'Mean Squared Error: {mse_neural}')\n\nEpoch 1/200\n 1/10 [==&gt;...........................] - ETA: 7s - loss: 347.8912\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 1s 18ms/step - loss: 294.9319 - val_loss: 244.3895\nEpoch 2/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 324.9168\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 232.8803 - val_loss: 197.6942\nEpoch 3/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 180.9170\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 183.4211 - val_loss: 159.6202\nEpoch 4/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 165.1238\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 143.9482 - val_loss: 124.9349\nEpoch 5/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 92.3229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 105.9881 - val_loss: 95.5513\nEpoch 6/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 82.3792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 74.0482 - val_loss: 72.0073\nEpoch 7/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 56.7143\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 49.9833 - val_loss: 55.6577\nEpoch 8/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 57.0857\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 35.1928 - val_loss: 46.6508\nEpoch 9/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 55.3334\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 27.1127 - val_loss: 43.1219\nEpoch 10/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 41.6672\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 24.8227 - val_loss: 41.6910\nEpoch 11/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 15.6358\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 24.0320 - val_loss: 39.6636\nEpoch 12/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 16.5143\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 23.3638 - val_loss: 37.3211\nEpoch 13/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 21.2830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 22.5541 - val_loss: 35.6362\nEpoch 14/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 16.3505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 21.8870 - val_loss: 34.0250\nEpoch 15/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 15.8477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 21.3093 - val_loss: 32.9343\nEpoch 16/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.8970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 20.7792 - val_loss: 32.0961\nEpoch 17/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 15.1547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 20.2345 - val_loss: 31.1544\nEpoch 18/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 17.8449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 19.7976 - val_loss: 30.5056\nEpoch 19/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 24.7252\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 19.2836 - val_loss: 29.7806\nEpoch 20/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 19.3199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 18.8552 - val_loss: 29.2129\nEpoch 21/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.2238\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 18.4406 - val_loss: 28.5988\nEpoch 22/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.6119\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 18.1526 - val_loss: 27.7706\nEpoch 23/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 11.9187\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 17.7112 - val_loss: 27.3806\nEpoch 24/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 11.6558\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 17.4135 - val_loss: 26.5879\nEpoch 25/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 8.2357\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 17.0842 - val_loss: 26.1701\nEpoch 26/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 15.2788\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 16.7184 - val_loss: 25.3978\nEpoch 27/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.9378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 16.4250 - val_loss: 24.8294\nEpoch 28/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 22.7182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 16.0773 - val_loss: 24.2736\nEpoch 29/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 9.6502\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 15.7886 - val_loss: 23.8289\nEpoch 30/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.4032\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 15.5297 - val_loss: 23.6357\nEpoch 31/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.5669\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 15.3171 - val_loss: 22.9437\nEpoch 32/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.5508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 15.0744 - val_loss: 22.5992\nEpoch 33/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 8.4463\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 14.7642 - val_loss: 22.1591\nEpoch 34/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 14.8626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 14.4969 - val_loss: 21.8559\nEpoch 35/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.3155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 14.2588 - val_loss: 21.4817\nEpoch 36/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 16.2426\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 14.0962 - val_loss: 21.1767\nEpoch 37/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 16.1637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 13.8706 - val_loss: 21.0124\nEpoch 38/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 8.4737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 13.5830 - val_loss: 20.5981\nEpoch 39/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 14.8893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 13.3536 - val_loss: 20.2631\nEpoch 40/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.3053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 13.1411 - val_loss: 20.0678\nEpoch 41/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 5.2634\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 12.9041 - val_loss: 19.7265\nEpoch 42/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.3662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 12.6987 - val_loss: 19.3800\nEpoch 43/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.0375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 12.4790 - val_loss: 19.1709\nEpoch 44/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.1693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 12.3415 - val_loss: 18.8776\nEpoch 45/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 18.1243\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 12.1331 - val_loss: 18.7619\nEpoch 46/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.9369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 11.9282 - val_loss: 18.6128\nEpoch 47/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 4.8273\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 11.7415 - val_loss: 18.2456\nEpoch 48/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.7153\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 11.5493 - val_loss: 18.0722\nEpoch 49/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 4.5620\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 11.3665 - val_loss: 17.7838\nEpoch 50/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.4269\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 11.2289 - val_loss: 17.3654\nEpoch 51/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 8.6888\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 11.0452 - val_loss: 17.2921\nEpoch 52/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.0735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 10.9149 - val_loss: 16.9543\nEpoch 53/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 11.9017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 10.7255 - val_loss: 16.4681\nEpoch 54/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 9.8202\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 10.6900 - val_loss: 16.4361\nEpoch 55/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 6.9314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 10.4187 - val_loss: 16.1096\nEpoch 56/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 6.1893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 10.3181 - val_loss: 15.7102\nEpoch 57/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 19.9282\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 10.1698 - val_loss: 15.5604\nEpoch 58/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.5112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.9940 - val_loss: 15.1299\nEpoch 59/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.6992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.8396 - val_loss: 14.6021\nEpoch 60/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.7258\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.6505 - val_loss: 14.3557\nEpoch 61/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 12.7889\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.4016 - val_loss: 14.0355\nEpoch 62/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 9.3175\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.1871 - val_loss: 13.4808\nEpoch 63/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 13.0129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.0255 - val_loss: 12.7705\nEpoch 64/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 9.5060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 9.1125 - val_loss: 12.7066\nEpoch 65/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.9731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 8.5621 - val_loss: 12.1827\nEpoch 66/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 8.8348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 8.4245 - val_loss: 12.0025\nEpoch 67/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.8515\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 8.1133 - val_loss: 11.5961\nEpoch 68/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 9.2272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 7.7905 - val_loss: 10.9517\nEpoch 69/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 11.3659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 7.4256 - val_loss: 10.5733\nEpoch 70/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 4.8542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 6.9310 - val_loss: 9.9457\nEpoch 71/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 8.4913\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 6.7352 - val_loss: 9.5454\nEpoch 72/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 5.6000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 6.2216 - val_loss: 8.8446\nEpoch 73/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.4833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 6.0779 - val_loss: 8.3606\nEpoch 74/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 4.6019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 5.6240 - val_loss: 7.6165\nEpoch 75/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 5.4904\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 5.2676 - val_loss: 7.1572\nEpoch 76/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 5.9965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 4.8687 - val_loss: 6.6257\nEpoch 77/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 7.1711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 4.2938 - val_loss: 5.7242\nEpoch 78/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 4.6984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 3.8105 - val_loss: 5.2429\nEpoch 79/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 5.5117\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 3.5048 - val_loss: 4.8230\nEpoch 80/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 3.6721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 3.1694 - val_loss: 4.6912\nEpoch 81/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.4475\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.9088 - val_loss: 4.4558\nEpoch 82/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 3.5710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.6437 - val_loss: 4.0943\nEpoch 83/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.5423 - val_loss: 3.9598\nEpoch 84/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.6611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.4155 - val_loss: 3.8857\nEpoch 85/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.3053 - val_loss: 3.9807\nEpoch 86/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.9128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.2105 - val_loss: 3.6619\nEpoch 87/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.1083\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 2.1851 - val_loss: 3.9521\nEpoch 88/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.5438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 2.0804 - val_loss: 3.7848\nEpoch 89/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.4609\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 2.0173 - val_loss: 3.6196\nEpoch 90/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.9157 - val_loss: 4.0561\nEpoch 91/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.8762 - val_loss: 3.9220\nEpoch 92/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.9918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.8017 - val_loss: 4.2967\nEpoch 93/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.4011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.7503 - val_loss: 4.1675\nEpoch 94/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2404\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.7000 - val_loss: 4.2174\nEpoch 95/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6584 - val_loss: 4.3136\nEpoch 96/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6422 - val_loss: 4.5664\nEpoch 97/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6601 - val_loss: 4.3826\nEpoch 98/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6021 - val_loss: 4.4954\nEpoch 99/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.6110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6093 - val_loss: 4.3542\nEpoch 100/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5915 - val_loss: 4.4917\nEpoch 101/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.9280\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5856 - val_loss: 4.3539\nEpoch 102/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2437\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.6913 - val_loss: 4.3722\nEpoch 103/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6667 - val_loss: 4.5514\nEpoch 104/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.3391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.6799 - val_loss: 4.2470\nEpoch 105/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.8972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5251 - val_loss: 4.2721\nEpoch 106/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.4904 - val_loss: 4.1500\nEpoch 107/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.9960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5101 - val_loss: 4.4245\nEpoch 108/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9842\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5415 - val_loss: 4.4869\nEpoch 109/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.6855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5682 - val_loss: 4.2810\nEpoch 110/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 1.5621 - val_loss: 4.7252\nEpoch 111/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.4223\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.4936 - val_loss: 4.0823\nEpoch 112/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.4263 - val_loss: 4.3356\nEpoch 113/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.9505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.4058 - val_loss: 4.3570\nEpoch 114/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1364\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.4004 - val_loss: 4.2286\nEpoch 115/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.7058\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.3788 - val_loss: 4.2550\nEpoch 116/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.3628 - val_loss: 4.1319\nEpoch 117/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.3525 - val_loss: 4.2877\nEpoch 118/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.3498 - val_loss: 4.0980\nEpoch 119/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.3558 - val_loss: 4.3987\nEpoch 120/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.3664 - val_loss: 4.0592\nEpoch 121/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.4243\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.3557 - val_loss: 4.1429\nEpoch 122/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5434\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.3587 - val_loss: 4.3011\nEpoch 123/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.2162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.3075 - val_loss: 4.1130\nEpoch 124/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.3062 - val_loss: 4.1875\nEpoch 125/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.8749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.2939 - val_loss: 4.1275\nEpoch 126/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.7022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.2862 - val_loss: 4.1439\nEpoch 127/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4748\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2636 - val_loss: 4.2123\nEpoch 128/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.8459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2923 - val_loss: 4.0778\nEpoch 129/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2366 - val_loss: 4.1418\nEpoch 130/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2420 - val_loss: 4.0191\nEpoch 131/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.6063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2328 - val_loss: 4.0821\nEpoch 132/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2160 - val_loss: 3.9873\nEpoch 133/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.3216\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2226 - val_loss: 3.9591\nEpoch 134/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2118 - val_loss: 3.9629\nEpoch 135/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2248 - val_loss: 3.9370\nEpoch 136/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.8327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2102 - val_loss: 4.1677\nEpoch 137/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.3105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2406 - val_loss: 3.9771\nEpoch 138/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.4184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2319 - val_loss: 3.9701\nEpoch 139/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6614\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.2023 - val_loss: 4.0695\nEpoch 140/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.1661 - val_loss: 3.8704\nEpoch 141/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.1620 - val_loss: 3.7812\nEpoch 142/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6653\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.1671 - val_loss: 4.0410\nEpoch 143/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1593\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.1929 - val_loss: 3.8743\nEpoch 144/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.8890\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 16ms/step - loss: 1.1339 - val_loss: 3.8330\nEpoch 145/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7323\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 11ms/step - loss: 1.1115 - val_loss: 4.2264\nEpoch 146/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0211\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 1.1832 - val_loss: 3.7758\nEpoch 147/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1883\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 1.1131 - val_loss: 3.9130\nEpoch 148/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6946\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 1.1353 - val_loss: 4.0194\nEpoch 149/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.1013 - val_loss: 3.8721\nEpoch 150/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0881 - val_loss: 3.7769\nEpoch 151/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.7583\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 1.1211 - val_loss: 3.9994\nEpoch 152/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.6702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 1.0880 - val_loss: 3.8293\nEpoch 153/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.8797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0664 - val_loss: 3.9481\nEpoch 154/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.3618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0860 - val_loss: 3.7232\nEpoch 155/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 2.2505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0807 - val_loss: 3.8856\nEpoch 156/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0721 - val_loss: 3.8596\nEpoch 157/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 1.0543 - val_loss: 3.6461\nEpoch 158/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0468 - val_loss: 3.8962\nEpoch 159/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.1104 - val_loss: 3.7324\nEpoch 160/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0721 - val_loss: 3.6628\nEpoch 161/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6144\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0883 - val_loss: 3.8767\nEpoch 162/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.3411\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 1.0200 - val_loss: 3.7122\nEpoch 163/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.3371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0072 - val_loss: 3.7466\nEpoch 164/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 1.0190 - val_loss: 3.7695\nEpoch 165/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 0.9936 - val_loss: 3.7561\nEpoch 166/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9933\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 0.9936 - val_loss: 3.5839\nEpoch 167/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.2651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 0.9797 - val_loss: 3.6750\nEpoch 168/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.8389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 5ms/step - loss: 0.9691 - val_loss: 3.5769\nEpoch 169/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 0.9671 - val_loss: 3.6106\nEpoch 170/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5273\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 9ms/step - loss: 0.9714 - val_loss: 3.6037\nEpoch 171/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.1818\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 9ms/step - loss: 0.9750 - val_loss: 3.4456\nEpoch 172/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5168\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 11ms/step - loss: 1.0639 - val_loss: 3.9225\nEpoch 173/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 11ms/step - loss: 1.0236 - val_loss: 3.5273\nEpoch 174/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0741\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 11ms/step - loss: 0.9308 - val_loss: 3.5890\nEpoch 175/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5207\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/10 [=======================&gt;......] - ETA: 0s - loss: 0.7738\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 19ms/step - loss: 0.9439 - val_loss: 3.6124\nEpoch 176/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 7/10 [====================&gt;.........] - ETA: 0s - loss: 0.9667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 23ms/step - loss: 0.9426 - val_loss: 3.6788\nEpoch 177/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 11ms/step - loss: 0.9169 - val_loss: 3.5571\nEpoch 178/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 10ms/step - loss: 0.9338 - val_loss: 3.5413\nEpoch 179/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7661\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 0.9521 - val_loss: 3.5513\nEpoch 180/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.5826\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 9ms/step - loss: 0.9362 - val_loss: 3.5677\nEpoch 181/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 0.9260 - val_loss: 3.6175\nEpoch 182/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.4228\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 0.9038 - val_loss: 3.3774\nEpoch 183/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.6833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 0.8910 - val_loss: 3.5205\nEpoch 184/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0754\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 10ms/step - loss: 0.9213 - val_loss: 3.4528\nEpoch 185/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.3477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 0.9082 - val_loss: 3.5272\nEpoch 186/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.9028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 0.9235 - val_loss: 3.5854\nEpoch 187/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 29ms/step - loss: 0.8802 - val_loss: 3.4181\nEpoch 188/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 13ms/step - loss: 0.8680 - val_loss: 3.3783\nEpoch 189/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 11ms/step - loss: 0.8712 - val_loss: 3.5081\nEpoch 190/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6739\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 12ms/step - loss: 0.8758 - val_loss: 3.3436\nEpoch 191/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.8637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 19ms/step - loss: 0.8614 - val_loss: 3.5361\nEpoch 192/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6165\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 12ms/step - loss: 0.8943 - val_loss: 3.6308\nEpoch 193/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.2859\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 10ms/step - loss: 0.8730 - val_loss: 3.3015\nEpoch 194/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 0.8727 - val_loss: 3.5948\nEpoch 195/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0321\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 0.8475 - val_loss: 3.2088\nEpoch 196/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.1546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 0.8609 - val_loss: 3.3008\nEpoch 197/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.9317\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 0.8234 - val_loss: 3.5025\nEpoch 198/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 6ms/step - loss: 0.8113 - val_loss: 3.2583\nEpoch 199/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.1787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 8ms/step - loss: 0.8164 - val_loss: 3.2970\nEpoch 200/200\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 1.0182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 7ms/step - loss: 0.7988 - val_loss: 3.4303\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 4ms/step\nMean Squared Error: 3.430276167557704\n\n\n\n\n\nThe neural network model is trained and validated, and the mean squared error is computed.\nConclusion\nIn conclusion, the advertising dataset provides a fascinating journey into the world of predictive analytics. From understanding data distributions to exploring correlations and implementing various regression techniques, we’ve gained valuable insights into the impact of advertising budgets on sales. Whether through linear regression, polynomial regression, or neural networks, the tools at our disposal allow us to make informed decisions.\nThis analysis serves as a foundation for further exploration and refinement, empowering marketers to optimize their strategies and maximize the impact of advertising budgets on sales.\n\n\n\n\nConnect with me on GitHub\n\n\nFind more projects and articles on my GitHub."
  },
  {
    "objectID": "posts/1. Probability theory and random variables/index.html",
    "href": "posts/1. Probability theory and random variables/index.html",
    "title": "Decoding Customer Behavior: An In-Depth Analysis with Naive Bayes Classifier (BLOG ON PROBABILITY THEORY AND RANDOM VARIABLES)",
    "section": "",
    "text": "Understanding customer behavior is a crucial aspect of strategic decision-making for businesses. In this blog post, we delve into a dataset containing information about 400 clients, exploring their demographics, purchasing decisions, and applying a Naive Bayes classifier to predict whether a customer will make a purchase based on their age and estimated salary.\nExploring the customer behavior dataset\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# Loading the dataset\ndf = pd.read_csv(\"Customer_Behaviour.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15624510\nMale\n19\n19000\n0\n\n\n1\n15810944\nMale\n35\n20000\n0\n\n\n2\n15668575\nFemale\n26\n43000\n0\n\n\n3\n15603246\nFemale\n27\n57000\n0\n\n\n4\n15804002\nMale\n19\n76000\n0\n\n\n5\n15728773\nMale\n27\n58000\n0\n\n\n6\n15598044\nFemale\n27\n84000\n0\n\n\n7\n15694829\nFemale\n32\n150000\n1\n\n\n8\n15600575\nMale\n25\n33000\n0\n\n\n9\n15727311\nFemale\n35\n65000\n0\n\n\n\n\n\n\n\n\n# Checking the shape and data types of the dataset\nprint(df.shape)\nprint(df.dtypes)\n\n(400, 5)\nUser ID             int64\nGender             object\nAge                 int64\nEstimatedSalary     int64\nPurchased           int64\ndtype: object\n\n\n\n# Descriptive statistics of the dataset\ndf.describe()\n\n\n\n\n\n\n\n\nUser ID\nAge\nEstimatedSalary\nPurchased\n\n\n\n\ncount\n4.000000e+02\n400.000000\n400.000000\n400.000000\n\n\nmean\n1.569154e+07\n37.655000\n69742.500000\n0.357500\n\n\nstd\n7.165832e+04\n10.482877\n34096.960282\n0.479864\n\n\nmin\n1.556669e+07\n18.000000\n15000.000000\n0.000000\n\n\n25%\n1.562676e+07\n29.750000\n43000.000000\n0.000000\n\n\n50%\n1.569434e+07\n37.000000\n70000.000000\n0.000000\n\n\n75%\n1.575036e+07\n46.000000\n88000.000000\n1.000000\n\n\nmax\n1.581524e+07\n60.000000\n150000.000000\n1.000000\n\n\n\n\n\n\n\n\n# Visualizing the data\n# Pairplot\nsns.pairplot(df, hue='Purchased', diag_kind='kde')\nplt.show()\n\n\n\n\nUnderstanding the correlation between features is essential in selecting appropriate variables for the models. We visualize this correlation using a heatmap:\n\n# Exclude non-numeric columns\nnumeric_columns = df.select_dtypes(include=[np.number]).columns\ncorr_matrix = df[numeric_columns].corr()\n\n# Visualize the correlation matrix\nsns.heatmap(corr_matrix, annot=True, linewidths=0.5)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n# Histograms and KDE for Age\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(df[df['Purchased'] == 1]['Age'], kde=True, color='blue', label='Purchased=1')\nsns.histplot(df[df['Purchased'] == 0]['Age'], kde=True, color='orange', label='Purchased=0')\nplt.title('Distribution of Age for Purchased/Not Purchased')\nplt.legend()\n\n# Histograms and KDE for Estimated Salary\nplt.subplot(1, 2, 2)\nsns.histplot(df[df['Purchased'] == 1]['EstimatedSalary'], kde=True, color='blue', label='Purchased=1')\nsns.histplot(df[df['Purchased'] == 0]['EstimatedSalary'], kde=True, color='orange', label='Purchased=0')\nplt.title('Distribution of Estimated Salary for Purchased/Not Purchased')\nplt.legend()\n\nplt.show()\n\n\n\n\nTo visualize the distribution of age and estimated salary separately for customers who made a purchase (Purchased=1) and those who did not (Purchased=0). The above plot provides a clearer understanding of the characteristics of customers in each category.\nProbability Theory and Random Variables\nNext, we apply probability theory to calculate the likelihood of customers making a purchase or not. The probabilities of purchase (Purchased=1) and no purchase (Purchased=0) provide a foundational understanding of the dataset. We then categorize age and salary into bins, allowing us to compute conditional probabilities for different age and salary groups.\n\n# Code for calculating probabilities\ntotal_samples = len(df)\np_purchase = len(df[df['Purchased'] == 1]) / total_samples\np_no_purchase = len(df[df['Purchased'] == 0]) / total_samples\n\nprint(f\"Probability of Purchase: {p_purchase:.2f}\")\nprint(f\"Probability of No Purchase: {p_no_purchase:.2f}\")\n\nProbability of Purchase: 0.36\nProbability of No Purchase: 0.64\n\n\n\n# Code for calculating conditional probabilities for 'Age' and 'EstimatedSalary'\n# Define age and salary categories\nage_bins = [0, 30, 40, 50, 60, np.inf]\nsalary_bins = [0, 50000, 100000, 150000, np.inf]\n\ndf['Age_Category'] = pd.cut(df['Age'], bins=age_bins, labels=['0-30', '30-40', '40-50', '50-60', '60+'])\ndf['Salary_Category'] = pd.cut(df['EstimatedSalary'], bins=salary_bins, labels=['0-50k', '50k-100k', '100k-150k', '150k+'])\n\n# Calculate conditional probabilities\ncond_prob_age_purchase = df.groupby('Age_Category', observed=False)['Purchased'].mean()\ncond_prob_salary_purchase = df.groupby('Salary_Category', observed=False)['Purchased'].mean()\n\nprint(\"Conditional Probabilities:\")\nprint(\"Age - Probability of Purchase:\")\nprint(cond_prob_age_purchase)\n\nConditional Probabilities:\nAge - Probability of Purchase:\nAge_Category\n0-30     0.054054\n30-40    0.232394\n40-50    0.602041\n50-60    0.918367\n60+           NaN\nName: Purchased, dtype: float64\n\n\n\nprint(\"\\nSalary - Probability of Purchase:\")\nprint(cond_prob_salary_purchase)\n\n\nSalary - Probability of Purchase:\nSalary_Category\n0-50k        0.354839\n50k-100k     0.182266\n100k-150k    0.849315\n150k+             NaN\nName: Purchased, dtype: float64\n\n\nNaive Bayes Classifier\nThe spotlight of our analysis is the implementation of a Naive Bayes classifier. We select ‘Age’ and ‘EstimatedSalary’ as features to predict the ‘Purchased’ outcome. After splitting the data into training and testing sets and standardizing the features, we train the classifier and evaluate its performance. The accuracy, confusion matrix, and classification report offer a comprehensive overview of the model’s predictive capabilities.\n\n# Feature selection\nX = df[['Age', 'EstimatedSalary']]\ny = df['Purchased']\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply Naive Bayes classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train_scaled, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test_scaled)\n\n# Evaluate the performance of the classifier\nprint(\"Naive Bayes Classifier Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n\nNaive Bayes Classifier Results:\nAccuracy: 0.94\n\n\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n\nConfusion Matrix:\n[[50  2]\n [ 3 25]]\n\n\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.94      0.96      0.95        52\n           1       0.93      0.89      0.91        28\n\n    accuracy                           0.94        80\n   macro avg       0.93      0.93      0.93        80\nweighted avg       0.94      0.94      0.94        80\n\n\n\nDecision Boundary and Probability Surface\nTo visualize the classifier’s decision-making process, we plot the decision boundary on a scatter plot of the training data. This boundary helps us understand how the model separates customers who make a purchase from those who do not. Additionally, we explore the probability surface, gaining insights into the regions where the model predicts a higher probability of purchase.\n\n# Function to plot decision boundary\ndef plot_decision_boundary(X, y, model, title):\n    h = .02  # Step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k', s=100)\n    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')  # Contour at the decision boundary\n    plt.title(title)\n    plt.xlabel('Age')\n    plt.ylabel('Estimated Salary')\n    plt.show()\n\n# Plot decision boundary of Naive Bayes classifier\nplot_decision_boundary(X_train_scaled, y_train, nb_classifier, \"Decision Boundary of Naive Bayes Classifier\")\n\n# Plotting probability surface\ndef plot_probability_surface(model, X, y):\n    proba = model.predict_proba(X)\n    plt.figure(figsize=(12, 6))\n\n    # Plotting decision boundary\n    plot_decision_boundary(X, y, model, \"Decision Boundary with Probabilities\")\n\n    # Contour plot for probability of Purchase\n    plt.contourf(X[:, 0], X[:, 1], proba[:, 1], levels=10, cmap=plt.cm.coolwarm, alpha=0.6)\n    plt.colorbar(label='Probability of Purchase')\n    plt.title(\"Probability Surface\")\n    plt.xlabel('Age')\n    plt.ylabel('Estimated Salary')\n    plt.show()\n\n\n\n\nReceiver Operating Characteristic (ROC) Curve\nNo analysis is complete without assessing the model’s ability to discriminate between positive and negative instances. We construct an ROC curve, illustrating the trade-off between true positive rate and false positive rate. The area under the ROC curve (AUC-ROC) serves as a quantitative measure of the model’s performance.\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, nb_classifier.predict_proba(X_test_scaled)[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nConclusion\nIn conclusion, our exploration of customer behavior data involving 400 clients has provided valuable insights into the factors influencing purchase decisions. Leveraging descriptive statistics and machine learning techniques, we gained a nuanced understanding of the dataset.\nThe mean age of customers was found to be approximately 37.65 years, with an average estimated salary of $69,742.50. Moreover, our analysis revealed a 36% probability of a customer making a purchase and a 64% probability of no purchase.\nThe application of a Naive Bayes classifier yielded remarkable results, with an accuracy rate of 94%. This signifies the model’s proficiency in correctly predicting whether a customer will make a purchase or not. Furthermore, the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC) reached an impressive 0.98, underscoring the classifier’s robust discriminatory capabilities.\nThese findings hold significant implications for businesses aiming to understand and predict customer behavior. By harnessing such insights, companies can tailor their marketing and sales strategies to effectively target potential customers, ultimately enhancing decision-making processes and driving business success.\n\n\n\n\n\nConnect with me on GitHub\n\n\nFind more projects and articles on my GitHub."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Gayatri Milind Bhatambarekar, a dedicated graduate student pursuing a Master’s in Computer Science at VT. I’m thrilled to invite you into my world of machine learning exploration through my blogs. Here, you’ll find a diverse range of topics, from unraveling the intricacies of probability theory and random variables to exploring clustering techniques, understanding regression models, delving into classification methods, and mastering anomaly/outlier detection. Join me on this learning journey, and let’s explore the fascinating domain of machine learning together. Feel free to connect, share your thoughts, and enjoy the exciting insights we uncover along the way. Happy reading!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Decoding Customer Behavior: An In-Depth Analysis with Naive Bayes Classifier (BLOG ON PROBABILITY THEORY AND RANDOM VARIABLES)\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nProbability Theory\n\n\nRandom Variables\n\n\nNaive Bayes Classifier\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nGayatri Milind Bhatambarekar\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling Customer Segmentation through RFM Analysis and K-Means Clustering (BLOG ON CLUSTERING)\n\n\n\n\n\n\n\nClustering\n\n\nK-Means Clustering\n\n\nRFM analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nGayatri Milind Bhatambarekar\n\n\n\n\n\n\n  \n\n\n\n\nA Comprehensive Analysis of Linear and Non-Linear Regression in Advertising Data (BLOG ON LINEAR AND NON LINEAR REGRESSION)\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nLinear Regression\n\n\nPolynomial Regression\n\n\nNeural Network Regression\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nGayatri Milind Bhatambarekar\n\n\n\n\n\n\n  \n\n\n\n\nExploring Diabetes Prediction Models: A Comparative Analysis (BLOG ON CLASSIFICATION)\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nClassification\n\n\nK-Nearest Neighbors\n\n\nSupport Vector Classifier\n\n\nDecision Tree Classifier\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nGayatri Milind Bhatambarekar\n\n\n\n\n\n\n  \n\n\n\n\nExploring Unsupervised Anomaly Detection in Thyroid Dataset (BLOG ON ANOMALY/OUTLIER DETECTION)\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nOutlier detection\n\n\nIsolation Forest\n\n\nZ-score\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nGayatri Milind Bhatambarekar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2. Clustering/index.html",
    "href": "posts/2. Clustering/index.html",
    "title": "Unveiling Customer Segmentation through RFM Analysis and K-Means Clustering (BLOG ON CLUSTERING)",
    "section": "",
    "text": "In the world of business, understanding your customers is paramount. It’s not just about the products or services you offer; it’s about knowing your customers’ preferences, behaviors, and how they engage with your brand. One powerful tool for gaining such insights is RFM analysis combined with K-Means clustering.\nExploring the Sales Dataset\nOur blog begins with exploring the sales dataset. We load the dataset, taking a sneak peek at the first 10 rows to get a sense of the data’s structure.\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the sales dataset\ndf = pd.read_csv('sales_data_sample.csv', encoding='unicode_escape')\n\n# Display the first few rows\ndf.head()\n\n\n\n\n\n\n\n\nORDERNUMBER\nQUANTITYORDERED\nPRICEEACH\nORDERLINENUMBER\nSALES\nORDERDATE\nSTATUS\nQTR_ID\nMONTH_ID\nYEAR_ID\n...\nADDRESSLINE1\nADDRESSLINE2\nCITY\nSTATE\nPOSTALCODE\nCOUNTRY\nTERRITORY\nCONTACTLASTNAME\nCONTACTFIRSTNAME\nDEALSIZE\n\n\n\n\n0\n10107\n30\n95.70\n2\n2871.00\n2/24/2003 0:00\nShipped\n1\n2\n2003\n...\n897 Long Airport Avenue\nNaN\nNYC\nNY\n10022\nUSA\nNaN\nYu\nKwai\nSmall\n\n\n1\n10121\n34\n81.35\n5\n2765.90\n5/7/2003 0:00\nShipped\n2\n5\n2003\n...\n59 rue de l'Abbaye\nNaN\nReims\nNaN\n51100\nFrance\nEMEA\nHenriot\nPaul\nSmall\n\n\n2\n10134\n41\n94.74\n2\n3884.34\n7/1/2003 0:00\nShipped\n3\n7\n2003\n...\n27 rue du Colonel Pierre Avia\nNaN\nParis\nNaN\n75508\nFrance\nEMEA\nDa Cunha\nDaniel\nMedium\n\n\n3\n10145\n45\n83.26\n6\n3746.70\n8/25/2003 0:00\nShipped\n3\n8\n2003\n...\n78934 Hillside Dr.\nNaN\nPasadena\nCA\n90003\nUSA\nNaN\nYoung\nJulie\nMedium\n\n\n4\n10159\n49\n100.00\n14\n5205.27\n10/10/2003 0:00\nShipped\n4\n10\n2003\n...\n7734 Strong St.\nNaN\nSan Francisco\nCA\nNaN\nUSA\nNaN\nBrown\nJulie\nMedium\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n# Checking the shape and data types of the dataset\nprint(df.shape)\n\n(2823, 25)\n\n\nThe dataset comprises various columns, including order numbers, quantities ordered, prices, order dates, customer details, and more. With 2823 rows, it offers a substantial amount of data for our analysis.\n\n# Descriptive statistics of the dataset\ndf.describe()\n\n\n\n\n\n\n\n\nORDERNUMBER\nQUANTITYORDERED\nPRICEEACH\nORDERLINENUMBER\nSALES\nQTR_ID\nMONTH_ID\nYEAR_ID\nMSRP\n\n\n\n\ncount\n2823.000000\n2823.000000\n2823.000000\n2823.000000\n2823.000000\n2823.000000\n2823.000000\n2823.00000\n2823.000000\n\n\nmean\n10258.725115\n35.092809\n83.658544\n6.466171\n3553.889072\n2.717676\n7.092455\n2003.81509\n100.715551\n\n\nstd\n92.085478\n9.741443\n20.174277\n4.225841\n1841.865106\n1.203878\n3.656633\n0.69967\n40.187912\n\n\nmin\n10100.000000\n6.000000\n26.880000\n1.000000\n482.130000\n1.000000\n1.000000\n2003.00000\n33.000000\n\n\n25%\n10180.000000\n27.000000\n68.860000\n3.000000\n2203.430000\n2.000000\n4.000000\n2003.00000\n68.000000\n\n\n50%\n10262.000000\n35.000000\n95.700000\n6.000000\n3184.800000\n3.000000\n8.000000\n2004.00000\n99.000000\n\n\n75%\n10333.500000\n43.000000\n100.000000\n9.000000\n4508.000000\n4.000000\n11.000000\n2004.00000\n124.000000\n\n\nmax\n10425.000000\n97.000000\n100.000000\n18.000000\n14082.800000\n4.000000\n12.000000\n2005.00000\n214.000000\n\n\n\n\n\n\n\nData Cleaning and Transformation\nBefore diving into analysis, we will first perform data cleaning and transformation. We drop unnecessary columns that won’t significantly contribute to our analysis, such as phone numbers and addresses.\n\n# Drop unnecessary columns\nto_drop = ['PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'STATE', 'POSTALCODE']\ndf = df.drop(to_drop, axis=1)\n\nAdditionally, we convert the ‘ORDERDATE’ column to a proper date format for easier handling later in our analysis.\n\n# Convert 'ORDERDATE' to datetime format\ndf['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])\n\nRFM Analysis: Understanding Customer Behavior\nRFM analysis involves three key metrics:\nRecency, Frequency, and MonetaryValue. These metrics provide a holistic view of customer behavior.\nRecency: Recency represents how recently a customer made a purchase. We calculate it by finding the number of days since the last purchase for each customer.\nFrequency: Frequency measures how often a customer makes a purchase. It is simply the count of orders placed by a customer.\nMonetaryValue: MonetaryValue is the total amount a customer has spent on purchases.\n\n# Calculate Recency, Frequency, and MonetaryValue for each customer\nlatest_date = df['ORDERDATE'].max() + pd.to_timedelta(1, 'D')  # Latest date in the dataset\n\ndf_RFM = df.groupby(['CUSTOMERNAME'])\ndf_RFM = df_RFM.agg({\n    'ORDERDATE': lambda x: (latest_date - x.max()).days,\n    'ORDERNUMBER': 'count',\n    'SALES': 'sum'\n})\n\n# Rename columns for clarity\ndf_RFM.rename(columns={'ORDERDATE': 'Recency', 'ORDERNUMBER': 'Frequency', 'SALES': 'MonetaryValue'}, inplace=True)\n\n# Display the first few rows of the RFM data\ndf_RFM.head()\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\n\n\nCUSTOMERNAME\n\n\n\n\n\n\n\nAV Stores, Co.\n196\n51\n157807.81\n\n\nAlpha Cognac\n65\n20\n70488.44\n\n\nAmica Models & Co.\n265\n26\n94117.26\n\n\nAnna's Decorations, Ltd\n84\n46\n153996.13\n\n\nAtelier graphique\n188\n7\n24179.96\n\n\n\n\n\n\n\nOur RFM data now provides insights into how recently customers made a purchase, how frequently they buy, and how much money they spend.\nData Exploration and Visualization\nThe scatter plot below illustrates the relationship between the quantity of items ordered and the corresponding total sales. Each point on the graph represents a transaction, allowing us to visually assess how changes in quantity impact overall sales.\n\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='QUANTITYORDERED', y='SALES', data=df, s=50)\nplt.title('Scatter Plot: Quantity Ordered vs Sales')\nplt.xlabel('Quantity Ordered')\nplt.ylabel('Sales')\nplt.show()\n\n\n\n\nIn the scatter plot depicted, we delve into the connection between the price of individual items and the resulting sales. This visualization helps us discern any patterns or trends in sales concerning the pricing of products.\n\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='PRICEEACH', y='SALES', data=df, s=50)\nplt.title('Scatter Plot: Price Each vs Sales')\nplt.xlabel('Price Each')\nplt.ylabel('Sales')\nplt.show()\n\n\n\n\nThe box plot provides a overview of sales distribution across different countries. Each box represents the interquartile range of sales within a specific country, offering insights into the variability and central tendencies of sales data.\n\nplt.figure(figsize=(9, 5))\nsns.boxplot(x='COUNTRY', y='SALES', data=df)\nplt.title('Box Plot: Sales by Country')\nplt.xlabel('Country')\nplt.ylabel('Sales')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n\n\n\nBefore diving into the clustering process, let’s explore our data visually. We create histograms to visualize the distribution of Recency, Frequency, and MonetaryValue.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Visualize the distribution of Recency, Frequency, and MonetaryValue\nplt.figure(figsize=(9, 5))\n\nplt.subplot(1, 3, 1)\nsns.histplot(df_RFM['Recency'], kde=True)\nplt.title('Distribution of Recency')\n\nplt.subplot(1, 3, 2)\nsns.histplot(df_RFM['Frequency'], kde=True)\nplt.title('Distribution of Frequency')\n\nplt.subplot(1, 3, 3)\nsns.histplot(df_RFM['MonetaryValue'], kde=True)\nplt.title('Distribution of MonetaryValue')\n\nplt.suptitle('Distribution of RFM Metrics')\nplt.show()\n\n\n\n\nThese histograms give us a glimpse into the distribution of Recency, Frequency, and MonetaryValue. We observe the skewness and decide to apply a log transformation to address this.\nLog Transformation and Standardization\nTo handle skewed data, we perform a log transformation on the RFM values. This not only helps in scaling the data but also in normalizing it.\n\nimport numpy as np\n\n# Log transformation of the RFM data\ndata_log = np.log(df_RFM)\n\n# Display the first few rows of the log-transformed data\ndata_log.head()\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\n\n\nCUSTOMERNAME\n\n\n\n\n\n\n\nAV Stores, Co.\n5.278115\n3.931826\n11.969133\n\n\nAlpha Cognac\n4.174387\n2.995732\n11.163204\n\n\nAmica Models & Co.\n5.579730\n3.258097\n11.452297\n\n\nAnna's Decorations, Ltd\n4.430817\n3.828641\n11.944683\n\n\nAtelier graphique\n5.236442\n1.945910\n10.093279\n\n\n\n\n\n\n\nNext, we standardize the log-transformed data using the StandardScaler.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit and transform the log-transformed data\ndata_normalized = scaler.fit_transform(data_log)\n\n# Create a DataFrame with the normalized data\ndata_normalized = pd.DataFrame(data_normalized, index=data_log.index, columns=data_log.columns)\n\n# Display summary statistics of the standardized data\ndata_normalized.describe().round(2)\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\n\n\n\n\ncount\n92.00\n92.00\n92.00\n\n\nmean\n0.00\n-0.00\n0.00\n\n\nstd\n1.01\n1.01\n1.01\n\n\nmin\n-3.51\n-3.67\n-3.82\n\n\n25%\n-0.24\n-0.41\n-0.39\n\n\n50%\n0.37\n0.06\n-0.04\n\n\n75%\n0.53\n0.45\n0.52\n\n\nmax\n1.12\n4.03\n3.92\n\n\n\n\n\n\n\nOur RFM data is now transformed, scaled, and ready for the next step: K-Means Clustering.\nK-Means Clustering: Unveiling Customer Segments\nK-Means clustering is a powerful technique to group similar data points into clusters. The elbow method helps us determine the optimal number of clusters.\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate SSE for different values of k (number of clusters)\nsse = {}\nfor k in range(1, 21):\n    kmeans = KMeans(n_clusters=k, random_state=1, n_init=10)  # Set n_init explicitly\n    kmeans.fit(data_normalized)\n    sse[k] = kmeans.inertia_\n\nplt.figure(figsize=(9, 5))\nplt.title('The Elbow Method')\n\n# Add X-axis label \"k\"\nplt.xlabel('k')\n\n# Add Y-axis label \"SSE\"\nplt.ylabel('SSE')\n\n#Plot SSE values for each key in the dictionary\nsns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n# plt.text(4.5, 60, \"Largest Angle\", bbox=dict(facecolor='lightgreen', alpha=0.5))\nplt.show()\n\n\n\n\nThe elbow method guides us in choosing the number of clusters. In this case, we observe an ‘elbow’ around 3 clusters, indicating a reasonable balance between model complexity and performance.\nNow, we proceed with K-Means clustering using 3 clusters.\n\n# Initialize KMeans with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=1, n_init=10)\n\n# Fit KMeans on the normalized data\nkmeans.fit(data_normalized)\n\n# Extract cluster labels\ncluster_labels = kmeans.labels_\n\n# Add cluster labels to the original RFM data\ndata_rfm = df_RFM.assign(Cluster=cluster_labels)\n\n# Display the first few rows of the clustered data\ndata_rfm.head()\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\nCluster\n\n\nCUSTOMERNAME\n\n\n\n\n\n\n\n\nAV Stores, Co.\n196\n51\n157807.81\n0\n\n\nAlpha Cognac\n65\n20\n70488.44\n0\n\n\nAmica Models & Co.\n265\n26\n94117.26\n0\n\n\nAnna's Decorations, Ltd\n84\n46\n153996.13\n0\n\n\nAtelier graphique\n188\n7\n24179.96\n1\n\n\n\n\n\n\n\nOur data is now enriched with cluster labels, revealing the segment to which each customer belongs.\nCluster Analysis: Deciphering Customer Characteristics\nWith customers grouped into clusters, we analyze each cluster’s characteristics, focusing on average Recency, Frequency, and MonetaryValue.\n\n# Group the data by cluster\ngrouped = data_rfm.groupby(['Cluster'])\n\n# Calculate average RFM values and segment sizes per cluster\ncluster_stats = grouped.agg({\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'MonetaryValue': ['mean', 'count']\n}).round(1)\n\n# Display the cluster statistics\ncluster_stats\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\n\n\n\nmean\nmean\nmean\ncount\n\n\nCluster\n\n\n\n\n\n\n\n\n0\n156.7\n30.4\n108231.7\n63\n\n\n1\n301.7\n13.7\n48611.9\n23\n\n\n2\n2.0\n99.0\n349326.5\n6\n\n\n\n\n\n\n\nThe cluster statistics showcase the average Recency, Frequency, and MonetaryValue for each cluster, providing insights into their unique characteristics.\nRelative Importance: What Sets Each Cluster Apart?\nTo understand what distinguishes each cluster, we calculate the relative importance of each RFM attribute within each cluster compared to the entire customer population.\n\n# Calculate relative importance of each attribute within each cluster\ncluster_avg = data_rfm.groupby(['Cluster']).mean()\npopulation_avg = df_RFM.mean()\n\n# Calculate relative importance\nrelative_imp = cluster_avg / population_avg - 1\n\n# Display relative importance scores\nrelative_imp.round(2)\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\n\n\nCluster\n\n\n\n\n\n\n\n0\n-0.14\n-0.01\n-0.01\n\n\n1\n0.65\n-0.55\n-0.55\n\n\n2\n-0.99\n2.23\n2.20\n\n\n\n\n\n\n\nThese relative importance scores shed light on how each cluster deviates from the overall customer population.\nVisualizing Customer Segmentation\nThe real power of our analysis comes when we visualize customer segmentation. We create scatter plots, pair plots, and even a 3D scatter plot to provide a comprehensive view.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Scatter plot of Recency vs Frequency with clusters\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='Recency', y='Frequency', hue='Cluster', data=data_rfm, palette='viridis', s=50)\nplt.title('K-Means Clustering: Recency vs Frequency')\nplt.xlabel('Recency (days)')\nplt.ylabel('Frequency')\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\n\n# Scatter plot of Frequency vs MonetaryValue with clusters\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='Frequency', y='MonetaryValue', hue='Cluster', data=data_rfm, palette='viridis', s=50)\nplt.title('K-Means Clustering: Frequency vs MonetaryValue')\nplt.xlabel('Frequency')\nplt.ylabel('MonetaryValue')\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\n\n# Scatter plot of Recency vs MonetaryValue with clusters\nplt.figure(figsize=(9, 5))\nsns.scatterplot(x='Recency', y='MonetaryValue', hue='Cluster', data=data_rfm, palette='viridis', s=50)\nplt.title('K-Means Clustering: Recency vs MonetaryValue')\nplt.xlabel('Recency (days)')\nplt.ylabel('MonetaryValue')\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\n\n# Pair plot with clusters\nsns.pairplot(data_rfm, hue='Cluster', palette='viridis', diag_kind='kde', height=3)\nplt.suptitle('Pair Plot with Clusters', y=1.02, size=16)\nplt.show()\n\n\n\n\n\n# 3D Scatter plot with clusters\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(data_rfm['Recency'], data_rfm['Frequency'], data_rfm['MonetaryValue'], c=data_rfm['Cluster'], cmap='viridis', s=50)\nax.set_xlabel('Recency (days)')\nax.set_ylabel('Frequency')\nax.set_zlabel('MonetaryValue')\nax.set_title('3D Scatter Plot with Clusters')\nplt.show()\n\n\n\n\nThese visualizations provide an intuitive understanding of how customers are segmented based on Recency, Frequency, and MonetaryValue.\nConclusion\nIn this blog of data exploration and analysis, we have unraveled valuable insights into customer behavior. Through RFM analysis and K-Means clustering, we’ve segmented customers into distinct groups, each with its own characteristics and relative importance.\nUnderstanding these customer segments empowers businesses to tailor their strategies, personalize marketing efforts, and enhance overall customer experience.\n\n\n\n\n\nConnect with me on GitHub\n\n\nFind more projects and articles on my GitHub."
  },
  {
    "objectID": "posts/4. Classification/index.html",
    "href": "posts/4. Classification/index.html",
    "title": "Exploring Diabetes Prediction Models: A Comparative Analysis (BLOG ON CLASSIFICATION)",
    "section": "",
    "text": "Diabetes is a widespread health concern affecting a large population globally. Identifying diabetes in its early stages poses a significant challenge, emphasizing the crucial role of advanced technologies in healthcare. Among these, machine learning models have emerged as invaluable tools for predicting the likelihood of diabetes onset. This blog focuses on three key classification algorithms—K-nearest neighbors (KNN), Support Vector Classifier (SVC), and Decision Tree Classifier (DTC). By understanding and comparing their strengths and nuances, we gain insights into the contributions of machine learning to healthcare, specifically in the context of diabetes identification.\nData Exploration and Preprocessing\nLet’s start by loading and exploring the diabetes dataset. We will examine the data’s structure, and summary statistics to understand the relationships between features.\n\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Loading the dataset\ndf = pd.read_csv('diabetes.csv')\n\n# Displaying the first 10 rows of the dataset\ndf.head(10)\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n5\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n6\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\n\n\n7\n10\n115\n0\n0\n0\n35.3\n0.134\n29\n0\n\n\n8\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n9\n8\n125\n96\n0\n0\n0.0\n0.232\n54\n1\n\n\n\n\n\n\n\n\n# Checking the shape and data types of the dataset\nprint(df.shape)\nprint(df.dtypes)\n\n(768, 9)\nPregnancies                   int64\nGlucose                       int64\nBloodPressure                 int64\nSkinThickness                 int64\nInsulin                       int64\nBMI                         float64\nDiabetesPedigreeFunction    float64\nAge                           int64\nOutcome                       int64\ndtype: object\n\n\n\n# Descriptive statistics of the dataset\ndf.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\n# Checking for missing values\ndf.isnull().any()\n\nPregnancies                 False\nGlucose                     False\nBloodPressure               False\nSkinThickness               False\nInsulin                     False\nBMI                         False\nDiabetesPedigreeFunction    False\nAge                         False\nOutcome                     False\ndtype: bool\n\n\nFeature Correlation\nUnderstanding the correlation between features is essential in selecting appropriate variables for the models. We visualize this correlation using a heatmap:\n\ncorrelation_matrix = df.corr()\nplt.figure(figsize=(8, 6 ))\nsns.heatmap(correlation_matrix, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Feature Correlation Heatmap')\nplt.show()\n\n\n\n\nThe 3D scatter plot with decision surfaces shown below visualizes the diabetes dataset. Each dot represents an individual, colored based on the likelihood of diabetes (Outcome). The three axes—Glucose, BMI, and Age—provide a multidimensional view, and hovering over the plot reveals the decision surfaces, offering insights into the complex relationships among these features.\n\nimport plotly.express as px\n\n# 3D Scatter plot with decision surfaces\nfig = px.scatter_3d(df, x='Glucose', y='BMI', z='Age', color='Outcome',\n                    opacity=0.7, size_max=10)\n\nfig.update_traces(marker=dict(size=4),\n                  selector=dict(mode='markers'))\n\nfig.update_layout(scene=dict(\n                    xaxis=dict(title='Glucose'),\n                    yaxis=dict(title='BMI'),\n                    zaxis=dict(title='Age'),\n                    ))\n\nfig.update_layout(title_text='3D Scatter Plot with Decision Surfaces')\nfig.show()\n\n\n                                                \n\n\nData Splitting\nTo train and evaluate our models, we split the dataset into training and testing sets:\n\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nprint(\"The shape of X_train is:\", X_train.shape)\nprint(\"The shape of X_test is:\", X_test.shape)\nprint(\"The shape of y_train is:\", y_train.shape)\nprint(\"The shape of y_test is:\", y_test.shape)\n\nThe shape of X_train is: (537, 8)\nThe shape of X_test is: (231, 8)\nThe shape of y_train is: (537,)\nThe shape of y_test is: (231,)\n\n\nK-Nearest Neighbors (KNN)\nKNN is a simple yet effective algorithm that classifies a data point based on the majority class of its k-nearest neighbors. The choice of ‘k’ determines the number of neighbors considered when making a prediction. This model is particularly intuitive, as it operates on the principle that similar instances in a feature space tend to belong to the same class.\nNow, let’s explore the first model - KNN:\n\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nknn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)  \nknn.fit(X_train, y_train)\n\n# Prediction and evaluation\ny_pred_knn = knn.predict(X_test)\n\nconfusion_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nprint(\"Confusion Matrix - KNN\")\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix_knn, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Class 0', 'Class 1'],\n            yticklabels=['Class 0', 'Class 1'])\nplt.title('Confusion Matrix - KNN')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nConfusion Matrix - KNN\n\n\n\n\n\n\n# Classification report and performance metrics\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(\"Classification Report - KNN\")\nprint(classification_report_knn)\n\nClassification Report - KNN\n              precision    recall  f1-score   support\n\n           0       0.79      0.88      0.83       146\n           1       0.74      0.59      0.65        85\n\n    accuracy                           0.77       231\n   macro avg       0.76      0.73      0.74       231\nweighted avg       0.77      0.77      0.76       231\n\n\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()\naccuracy  =(tp+tn)/(tp+tn+fp+fn)\nprecision =(tp)/(tp+fp)\nrecall  =(tp)/(tp+fn)\nf1 =2*(( precision* recall)/( precision + recall))\n\nprint('Accuracy:\\t',accuracy*100,\n    '\\nPrecision:\\t',precision*100,\n    '\\nRecall: \\t',recall*100,\n    '\\nF1-Score:\\t',f1*100)\n\nAccuracy:    77.05627705627705 \nPrecision:   73.52941176470588 \nRecall:      58.82352941176471 \nF1-Score:    65.359477124183\n\n\n\n# ROC Curve and Precision-Recall Curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_knn)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve - KNN')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_knn)\n\nplt.figure(figsize=(8, 6))\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall Curve - KNN')\nplt.show()\n\n\n\n\nSupport Vector Classifier (SVC)\nSVC is a powerful model for both classification and regression tasks. It works by finding a hyperplane that best separates the data points into different classes. The flexibility of SVC lies in its ability to handle complex decision boundaries, making it suitable for scenarios where data is not linearly separable.\n\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\n\n# Initializing and fitting the SVC model\nm = SVC(C=1)\nm.fit(X_train, y_train)\ny_pred_svc = m.predict(X_test)\n\n# Confusion matrix\nconfusion_matrix_svc = confusion_matrix(y_test, y_pred_svc)\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix_svc, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Class 0', 'Class 1'],\n            yticklabels=['Class 0', 'Class 1'])\nplt.title('Confusion Matrix - SVC')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n\n\n\n\nclassification_report_svc = classification_report(y_test, y_pred_svc)\nprint(\"Classification Report - SVC\")\nprint(classification_report_svc)\n\nClassification Report - SVC\n              precision    recall  f1-score   support\n\n           0       0.75      0.95      0.84       146\n           1       0.83      0.47      0.60        85\n\n    accuracy                           0.77       231\n   macro avg       0.79      0.71      0.72       231\nweighted avg       0.78      0.77      0.75       231\n\n\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_svc).ravel()\naccuracysvc  =(tp+tn)/(tp+tn+fp+fn)\nprecisionsvc =(tp)/(tp+fp)\nrecallsvc =(tp)/(tp+fn)\nf1svc =2*(( precisionsvc* recallsvc)/( precisionsvc + recallsvc))\n\nprint('Accuracy:\\t',accuracysvc*100,\n    '\\nPrecision:\\t',precisionsvc*100,\n    '\\nRecall: \\t',recallsvc*100,\n    '\\nF1-Score:\\t',f1svc*100)\n\nAccuracy:    77.05627705627705 \nPrecision:   83.33333333333334 \nRecall:      47.05882352941176 \nF1-Score:    60.150375939849624\n\n\n\n# ROC Curve and Precision-Recall Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_svc)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve - SVC')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_svc)\n\nplt.figure(figsize=(8, 6))\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall Curve - SVC')\nplt.show()\n\n\n\n\nDecision Tree Classifier\nFinally, exploring the Decision Tree model: Decision trees are tree-like models where each internal node represents a decision based on the value of a particular feature. These decisions lead to different branches, eventually reaching leaf nodes that represent the class labels. Decision trees are easy to interpret and visualize, making them a popular choice for various applications.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Initializing and fitting the Decision Tree model\ndclf = DecisionTreeClassifier()\ndclf.fit(X_train, y_train)\n\n# Prediction and evaluation\ny_pred_dt = dclf.predict(X_test)\n\n# Confusion matrix \nconfusion_matrix_dt = confusion_matrix(y_test, y_pred_dt)\nprint(\"Confusion Matrix - DT\")\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix_dt, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Class 0', 'Class 1'],\n            yticklabels=['Class 0', 'Class 1'])\nplt.title('Confusion Matrix - DT')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nConfusion Matrix - DT\n\n\n\n\n\n\n# Classification report and performance metrics\nclassification_report_dt = classification_report(y_test, y_pred_dt)\nprint(\"Classification Report - DT\")\nprint(classification_report_dt)\n\nClassification Report - DT\n              precision    recall  f1-score   support\n\n           0       0.73      0.79      0.76       146\n           1       0.59      0.51      0.54        85\n\n    accuracy                           0.69       231\n   macro avg       0.66      0.65      0.65       231\nweighted avg       0.68      0.69      0.68       231\n\n\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred_dt).ravel()\naccuracydt  =(tp+tn)/(tp+tn+fp+fn)\nprecisiondt =(tp)/(tp+fp)\nrecalldt =(tp)/(tp+fn)\nf1dt =2*(( precisiondt* recalldt)/( precisiondt + recalldt))\n\nprint('Accuracy:\\t',accuracydt*100,\n    '\\nPrecision:\\t',precisiondt*100,\n    '\\nRecall: \\t',recalldt*100,\n    '\\nF1-Score:\\t',f1dt*100)\n\nAccuracy:    68.83116883116884 \nPrecision:   58.9041095890411 \nRecall:      50.588235294117645 \nF1-Score:    54.430379746835435\n\n\n\n# ROC Curve and Precision-Recall Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_dt)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve - DT')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_dt)\n\nplt.figure(figsize=(8, 6))\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall Curve - DT')\nplt.show()\n\n\n\n\nModel Comparison Analysis Let’s analyze and compare the performance metrics of each model to gain insights into their effectiveness in predicting diabetes.\n\nplt.figure(figsize=(8, 6))\n\n# KNN\nfpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_knn)\nroc_auc_knn = auc(fpr_knn, tpr_knn)\nplt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {roc_auc_knn:.2f})')\n\n# SVC\nfpr_svc, tpr_svc, _ = roc_curve(y_test, y_pred_svc)\nroc_auc_svc = auc(fpr_svc, tpr_svc)\nplt.plot(fpr_svc, tpr_svc, label=f'SVC (AUC = {roc_auc_svc:.2f})')\n\n# Decision Tree\nfpr_dtc, tpr_dtc, _ = roc_curve(y_test, y_pred_dt)\nroc_auc_dtc = auc(fpr_dtc, tpr_dtc)\nplt.plot(fpr_dtc, tpr_dtc, label=f'DT(AUC = {roc_auc_dtc:.2f})')\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Model Comparison')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\n# KNN\nprecision_knn, recall_knn, _ = precision_recall_curve(y_test, y_pred_knn)\nplt.plot(recall_knn, precision_knn, label=f'KNN')\n\n# SVC\nprecision_svc, recall_svc, _ = precision_recall_curve(y_test, y_pred_svc)\nplt.plot(recall_svc, precision_svc, label=f'SVC')\n\n# Decision Tree\nprecision_dtc, recall_dtc, _ = precision_recall_curve(y_test, y_pred_dt)\nplt.plot(recall_dtc, precision_dtc, label=f'Decision Tree')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall Curve - Model Comparison')\nplt.legend()\nplt.show()\n\n\n\n\nComparison Analysis\nAccuracy: KNN and SVC show similar accuracy, both around 77%, while Decision Tree lags slightly behind at 68%.\nPrecision: SVC has the highest precision at 83.33%, indicating a low rate of false positives. KNN follows with 73.53%, and Decision Tree has the lowest precision at 58.33%.\nRecall: KNN has the highest recall at 58.82%, followed by Decision Tree (49.41%), and SVC with the lowest recall at 47.06%.\nF1-Score: KNN achieves the highest F1-score at 65.36%, followed by SVC (60.15%), and Decision Tree with a slightly lower F1-score of 53.58%.\nConclusion\nIn conclusion, the choice of a diabetes prediction model depends on specific goals and priorities. For instance, if minimizing false positives is crucial, the higher precision of SVC might be favored. If balancing precision and recall is important, KNN could be a reasonable choice with its relatively balanced performance. KNN excels in simplicity and interpretability, SVC in handling complex decision boundaries, and Decision Trees in providing a clear decision-making structure. The detailed analysis and graphical representation empower decision-makers to choose the model that aligns with the unique requirements of the prediction task.\n\n  –&gt;\n\n\nConnect with me on GitHub\n\n\nFind more projects and articles on my GitHub."
  }
]